{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                     # Llibreria matemÃ tica\n",
    "import matplotlib.pyplot as plt        # Per mostrar plots\n",
    "import sklearn                         # Llibreia de DM\n",
    "import sklearn.datasets as ds            # Per carregar mÃ©s facilment el dataset digits\n",
    "import sklearn.model_selection as cv    # Pel Cross-validation\n",
    "import sklearn.neighbors as nb           # Per fer servir el knn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np    # Numeric and matrix computation\n",
    "import pandas as pd   # Optional: good package for manipulating data \n",
    "import sklearn as sk  # Package with learning algorithms implemented\n",
    "\n",
    "# Loading the dataset.\n",
    "df = pd.read_csv(\"Train.csv\")\n",
    "y=df['readmitted'].values\n",
    "X=df.values[:,0:71].astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's separate the data into training (for adjusting parameters), and validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 5428, 1: 42525}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, X_test,  y_train, y_test) = cv.train_test_split(X, y, test_size=.3, stratify = y,random_state=1)\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is entirely numerical, but values are not normalized. We proceed to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#scaler = StandardScaler().fit(X_train)\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1)).fit(X_train)\n",
    "\n",
    "# Apply the normalization trained in training data in both training and test sets\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset has a lot of rows and is highly imbalanced, so we perform an undersampling of the train dataset but preserving the ratio of the classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 {0: 500, 1: 3917}\n",
      "Remaining {0: 4928, 1: 38608}\n",
      "Split 1 {0: 500, 1: 3917}\n",
      "Remaining {0: 4428, 1: 34691}\n",
      "Split 2 {0: 500, 1: 3917}\n",
      "Remaining {0: 3928, 1: 30774}\n",
      "Split 3 {0: 500, 1: 3917}\n",
      "Remaining {0: 3428, 1: 26857}\n",
      "Split 4 {0: 500, 1: 3917}\n",
      "Remaining {0: 2928, 1: 22940}\n"
     ]
    }
   ],
   "source": [
    "ratio = counts[1]/counts[0]\n",
    "num_samples_0 = 500;\n",
    "num_samples_1 = int(num_samples_0*ratio)\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(sampling_strategy={0:num_samples_0,1:num_samples_1},random_state=42)\n",
    "\n",
    "splits = 5\n",
    "X_train_splits = []\n",
    "y_train_splits = []\n",
    "\n",
    "for i in range(splits):\n",
    "    X_train_r, y_train_r = rus.fit_resample(X_train, y_train)\n",
    "    X_train_splits.append(X_train_r)\n",
    "    y_train_splits.append(y_train_r)\n",
    "    indices = rus.sample_indices_\n",
    "    X_train = np.delete(X_train,indices,0)\n",
    "    y_train = np.delete(y_train,indices,0)\n",
    "    unique, counts = np.unique(y_train_r, return_counts=True)\n",
    "    print(\"Split\",i,dict(zip(unique, counts)))\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    print(\"Remaining\",dict(zip(unique, counts)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting classifier\n",
    "Custom implementation of a voting classifier for fitted classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VotingClassifier(object):\n",
    "    \"\"\" Implements a voting classifier for pre-trained classifiers\"\"\"\n",
    "\n",
    "    def __init__(self, estimators):\n",
    "        self.estimators = estimators\n",
    "\n",
    "    def predict(self, X):\n",
    "        # get values\n",
    "        Y = np.zeros([X.shape[0], len(self.estimators)], dtype=int)\n",
    "        for i, clf in enumerate(self.estimators):\n",
    "            Y[:, i] = clf.predict(X)\n",
    "        # apply voting \n",
    "        y = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            y[i] = np.argmax(np.bincount(Y[i,:]))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM\n",
    "\n",
    "Let's try an SVM with default parameters. Linear means that we are not using any kernel to move the data to a higher dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix on test set:\n",
      " [[    0  2327]\n",
      " [    0 18225]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      2327\n",
      "           1       0.89      1.00      0.94     18225\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     20552\n",
      "   macro avg       0.44      0.50      0.47     20552\n",
      "weighted avg       0.79      0.89      0.83     20552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "svms = []\n",
    "for i in range(splits):\n",
    "    svm = SVC(kernel='linear')\n",
    "    svm.fit(X_train_splits[i],y_train_splits[i])\n",
    "    svms.append(svm)\n",
    "\n",
    "vc = VotingClassifier(svms)\n",
    "pred=vc.predict(X_test)\n",
    "print(\"Confusion matrix on test set:\\n\",sklearn.metrics.confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are really bad, so this classifier doesn't work. However, we can try to adjust the weight of each class to compensate the classes being unbalanced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix on test set:\n",
      " [[ 1219  1108]\n",
      " [ 5482 12743]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.52      0.27      2327\n",
      "           1       0.92      0.70      0.79     18225\n",
      "\n",
      "   micro avg       0.68      0.68      0.68     20552\n",
      "   macro avg       0.55      0.61      0.53     20552\n",
      "weighted avg       0.84      0.68      0.74     20552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svms = []\n",
    "for i in range(splits):\n",
    "    svm = SVC(kernel='linear', class_weight='balanced')\n",
    "    svm.fit(X_train_splits[i],y_train_splits[i])\n",
    "    svms.append(svm)\n",
    "\n",
    "vc = VotingClassifier(svms)\n",
    "pred=vc.predict(X_test)\n",
    "print(\"Confusion matrix on test set:\\n\",sklearn.metrics.confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice a huge improvement in recall of class 0, which was our goal. Therefore, we will stick with these weights. However, the linear SVM has parameter C that has to be adjusted. We will use *GridSearch* method to find the optimal value of C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split 0\n",
      "Best value of parameter C found:  {'C': 0.01}\n",
      "Recall 10-fold cross mean on train data = 0.508\n",
      "Number of supports:  3970 ( 0 of them have slacks)\n",
      "Prop. of supports:  0.8988000905592031\n",
      "\n",
      "Split 1\n",
      "Best value of parameter C found:  {'C': 0.01}\n",
      "Recall 10-fold cross mean on train data = 0.526\n",
      "Number of supports:  3880 ( 0 of them have slacks)\n",
      "Prop. of supports:  0.8784242698664252\n",
      "\n",
      "Split 2\n",
      "Best value of parameter C found:  {'C': 0.01}\n",
      "Recall 10-fold cross mean on train data = 0.502\n",
      "Number of supports:  3874 ( 0 of them have slacks)\n",
      "Prop. of supports:  0.87706588182024\n",
      "\n",
      "Split 3\n",
      "Best value of parameter C found:  {'C': 0.01}\n",
      "Recall 10-fold cross mean on train data = 0.5720000000000001\n",
      "Number of supports:  3740 ( 0 of them have slacks)\n",
      "Prop. of supports:  0.8467285487887707\n",
      "\n",
      "Split 4\n",
      "Best value of parameter C found:  {'C': 10.0}\n",
      "Recall 10-fold cross mean on train data = 0.508\n",
      "Number of supports:  3520 ( 0 of them have slacks)\n",
      "Prop. of supports:  0.7969209870953136\n",
      "\n",
      "Confusion matrix on test set:\n",
      " [[ 1233  1094]\n",
      " [ 5741 12484]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.53      0.27      2327\n",
      "           1       0.92      0.68      0.79     18225\n",
      "\n",
      "   micro avg       0.67      0.67      0.67     20552\n",
      "   macro avg       0.55      0.61      0.53     20552\n",
      "weighted avg       0.84      0.67      0.73     20552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Cs = np.logspace(-2, 2, num=5, base=10.0)\n",
    "param_grid = {'C': Cs}\n",
    "\n",
    "svms = []\n",
    "for i in range(splits):\n",
    "    print(\"\\nSplit\",i)\n",
    "    \n",
    "    scorer = make_scorer(f1_score,pos_label=0)\n",
    "    grid_search = GridSearchCV(SVC(kernel='linear',class_weight='balanced'), param_grid, cv=10, scoring=scorer, iid=True)\n",
    "    grid_search.fit(X_train_splits[i],y_train_splits[i])\n",
    "\n",
    "    parval=grid_search.best_params_\n",
    "    print(\"Best value of parameter C found: \",parval)\n",
    "    \n",
    "    scorer = make_scorer(recall_score,pos_label=0)\n",
    "    cvacc = cross_val_score(SVC(C=parval['C'],kernel='linear',class_weight='balanced'), X=X_train_splits[i],  y=y_train_splits[i], cv=10, scoring=scorer)\n",
    "    print('Recall 10-fold cross mean on train data =', cvacc.mean())\n",
    "\n",
    "    svm = SVC(C=parval['C'],kernel='linear',class_weight='balanced')\n",
    "    svm.fit(X_train_splits[i], y_train_splits[i])\n",
    "    svms.append(svm)\n",
    "    \n",
    "    print(\"Number of supports: \",np.sum(svm.n_support_), \"(\",np.sum(np.abs(svm.dual_coef_)==parval['C']) ,\"of them have slacks)\")\n",
    "    print(\"Prop. of supports: \",np.sum(svm.n_support_)/X_train_splits[i].shape[0])\n",
    "    \n",
    "vc = VotingClassifier(svms)\n",
    "pred=vc.predict(X_test)\n",
    "print(\"\\nConfusion matrix on test set:\\n\",sklearn.metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial kernels\n",
    "\n",
    "After trying with linear SVMs, the next step is using SVMs with a kernel. We'll try first polynomial kernel with degree 2 with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix on test set:\n",
      " [[ 1233  1094]\n",
      " [ 5664 12561]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.53      0.27      2327\n",
      "           1       0.92      0.69      0.79     18225\n",
      "\n",
      "   micro avg       0.67      0.67      0.67     20552\n",
      "   macro avg       0.55      0.61      0.53     20552\n",
      "weighted avg       0.84      0.67      0.73     20552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svms = []\n",
    "for i in range(splits):\n",
    "    svm = SVC(kernel='poly', degree=2, gamma='auto', class_weight='balanced')\n",
    "    svm.fit(X_train_splits[i],y_train_splits[i])\n",
    "    svms.append(svm)\n",
    "\n",
    "vc = VotingClassifier(svms)\n",
    "pred=vc.predict(X_test)\n",
    "print(\"Confusion matrix on test set:\\n\",sklearn.metrics.confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is similar to the linear SVMs. Now, let's try to find the best C parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split 0\n",
      "Best value of parameter C found:  {'C': 0.1}\n",
      "Recall 10-fold cross mean on train data = 0.43600000000000005\n",
      "Number of supports:  4211 ( 0 of them have slacks)\n",
      "Prop. of supports:  0.9533620104143083\n",
      "\n",
      "Split 1\n",
      "Best value of parameter C found:  {'C': 1.0}\n",
      "Recall 10-fold cross mean on train data = 0.53\n",
      "Number of supports:  3756 ( 0 of them have slacks)\n",
      "Prop. of supports:  0.8503509169119312\n",
      "\n",
      "Split 2\n",
      "Best value of parameter C found:  {'C': 1.0}\n",
      "Recall 10-fold cross mean on train data = 0.48599999999999993\n",
      "Number of supports:  3765 ( 0 of them have slacks)\n",
      "Prop. of supports:  0.8523884989812089\n",
      "\n",
      "Split 3\n",
      "Best value of parameter C found:  {'C': 1.0}\n",
      "Recall 10-fold cross mean on train data = 0.5519999999999999\n",
      "Number of supports:  3629 ( 0 of them have slacks)\n",
      "Prop. of supports:  0.8215983699343445\n",
      "\n",
      "Split 4\n",
      "Best value of parameter C found:  {'C': 1.0}\n",
      "Recall 10-fold cross mean on train data = 0.502\n",
      "Number of supports:  3694 ( 0 of them have slacks)\n",
      "Prop. of supports:  0.8363142404346842\n",
      "\n",
      "Confusion matrix on test set:\n",
      " [[ 1242  1085]\n",
      " [ 5745 12480]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.53      0.27      2327\n",
      "           1       0.92      0.68      0.79     18225\n",
      "\n",
      "   micro avg       0.67      0.67      0.67     20552\n",
      "   macro avg       0.55      0.61      0.53     20552\n",
      "weighted avg       0.84      0.67      0.73     20552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Cs = np.logspace(-2, 2, num=5, base=10.0)\n",
    "param_grid = {'C': Cs}\n",
    "\n",
    "degree = 2\n",
    "\n",
    "svms = []\n",
    "for i in range(splits):\n",
    "    print(\"\\nSplit\",i)\n",
    "    \n",
    "    scorer = make_scorer(f1_score,pos_label=0)\n",
    "    grid_search = GridSearchCV(SVC(kernel='poly',degree=degree,gamma='auto',class_weight='balanced'), param_grid, cv=10, scoring=scorer, iid=True)\n",
    "    grid_search.fit(X_train_splits[i],y_train_splits[i])\n",
    "\n",
    "    parval=grid_search.best_params_\n",
    "    print(\"Best value of parameter C found: \",parval)\n",
    "    \n",
    "    scorer = make_scorer(recall_score,pos_label=0)\n",
    "    cvacc = cross_val_score(SVC(C=parval['C'],kernel='poly',degree=degree,gamma='auto',class_weight='balanced'), X=X_train_splits[i],  y=y_train_splits[i], cv=10, scoring=scorer)\n",
    "    print('Recall 10-fold cross mean on train data =', cvacc.mean())\n",
    "\n",
    "    svm = SVC(C=parval['C'],kernel='poly',degree=degree,gamma='auto',class_weight='balanced')\n",
    "    svm.fit(X_train_splits[i], y_train_splits[i])\n",
    "    svms.append(svm)\n",
    "    \n",
    "    print(\"Number of supports: \",np.sum(svm.n_support_), \"(\",np.sum(np.abs(svm.dual_coef_)==parval['C']) ,\"of them have slacks)\")\n",
    "    print(\"Prop. of supports: \",np.sum(svm.n_support_)/X_train_splits[i].shape[0])\n",
    "    \n",
    "vc = VotingClassifier(svms)\n",
    "pred=vc.predict(X_test)\n",
    "print(\"\\nConfusion matrix on test set:\\n\",sklearn.metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try with degree 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix on test set:\n",
      " [[ 1198  1129]\n",
      " [ 5586 12639]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.51      0.26      2327\n",
      "           1       0.92      0.69      0.79     18225\n",
      "\n",
      "   micro avg       0.67      0.67      0.67     20552\n",
      "   macro avg       0.55      0.60      0.53     20552\n",
      "weighted avg       0.83      0.67      0.73     20552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svms = []\n",
    "for i in range(splits):\n",
    "    svm = SVC(kernel='poly', degree=3, gamma='auto', class_weight='balanced')\n",
    "    svm.fit(X_train_splits[i],y_train_splits[i])\n",
    "    svms.append(svm)\n",
    "\n",
    "vc = VotingClassifier(svms)\n",
    "pred=vc.predict(X_test)\n",
    "print(\"Confusion matrix on test set:\\n\",sklearn.metrics.confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best C parameter for degree 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split 0\n",
      "Best value of parameter C found:  {'C': 0.1}\n",
      "Recall 10-fold cross mean on train data = 0.44000000000000006\n",
      "Number of supports:  4205 ( 0 of them have slacks)\n",
      "Prop. of supports:  0.9520036223681232\n",
      "\n",
      "Split 1\n",
      "Best value of parameter C found:  {'C': 1.0}\n",
      "Recall 10-fold cross mean on train data = 0.514\n",
      "Number of supports:  3700 ( 0 of them have slacks)\n",
      "Prop. of supports:  0.8376726284808693\n",
      "\n",
      "Split 2\n",
      "Best value of parameter C found:  {'C': 1.0}\n",
      "Recall 10-fold cross mean on train data = 0.47000000000000003\n",
      "Number of supports:  3712 ( 0 of them have slacks)\n",
      "Prop. of supports:  0.8403894045732397\n",
      "\n",
      "Split 3\n",
      "Best value of parameter C found:  {'C': 1.0}\n",
      "Recall 10-fold cross mean on train data = 0.5280000000000001\n",
      "Number of supports:  3591 ( 0 of them have slacks)\n",
      "Prop. of supports:  0.8129952456418383\n",
      "\n",
      "Split 4\n",
      "Best value of parameter C found:  {'C': 1.0}\n",
      "Recall 10-fold cross mean on train data = 0.506\n",
      "Number of supports:  3653 ( 0 of them have slacks)\n",
      "Prop. of supports:  0.8270319221190854\n",
      "\n",
      "Confusion matrix on test set:\n",
      " [[ 1203  1124]\n",
      " [ 5615 12610]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.52      0.26      2327\n",
      "           1       0.92      0.69      0.79     18225\n",
      "\n",
      "   micro avg       0.67      0.67      0.67     20552\n",
      "   macro avg       0.55      0.60      0.53     20552\n",
      "weighted avg       0.83      0.67      0.73     20552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Cs = np.logspace(-2, 2, num=5, base=10.0)\n",
    "param_grid = {'C': Cs}\n",
    "\n",
    "degree = 3\n",
    "\n",
    "svms = []\n",
    "for i in range(splits):\n",
    "    print(\"\\nSplit\",i)\n",
    "    \n",
    "    scorer = make_scorer(f1_score,pos_label=0)\n",
    "    grid_search = GridSearchCV(SVC(kernel='poly',degree=degree,gamma='auto',class_weight='balanced'), param_grid, cv=10, scoring=scorer, iid=True)\n",
    "    grid_search.fit(X_train_splits[i],y_train_splits[i])\n",
    "\n",
    "    parval=grid_search.best_params_\n",
    "    print(\"Best value of parameter C found: \",parval)\n",
    "    \n",
    "    scorer = make_scorer(recall_score,pos_label=0)\n",
    "    cvacc = cross_val_score(SVC(C=parval['C'],kernel='poly',degree=degree,gamma='auto',class_weight='balanced'), X=X_train_splits[i],  y=y_train_splits[i], cv=10, scoring=scorer)\n",
    "    print('Recall 10-fold cross mean on train data =', cvacc.mean())\n",
    "\n",
    "    svm = SVC(C=parval['C'],kernel='poly',degree=degree,gamma='auto',class_weight='balanced')\n",
    "    svm.fit(X_train_splits[i], y_train_splits[i])\n",
    "    svms.append(svm)\n",
    "    \n",
    "    print(\"Number of supports: \",np.sum(svm.n_support_), \"(\",np.sum(np.abs(svm.dual_coef_)==parval['C']) ,\"of them have slacks)\")\n",
    "    print(\"Prop. of supports: \",np.sum(svm.n_support_)/X_train_splits[i].shape[0])\n",
    "    \n",
    "vc = VotingClassifier(svms)\n",
    "pred=vc.predict(X_test)\n",
    "print(\"\\nConfusion matrix on test set:\\n\",sklearn.metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBF Kernel\n",
    "There's another possibility for the kernel: The RBF kernel. This is the default kernel in the implementation of SVMs in sklearn, so we don't need to explicitely say the kernel used. Let's try it with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix on test set:\n",
      " [[ 1216  1111]\n",
      " [ 5554 12671]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.52      0.27      2327\n",
      "           1       0.92      0.70      0.79     18225\n",
      "\n",
      "   micro avg       0.68      0.68      0.68     20552\n",
      "   macro avg       0.55      0.61      0.53     20552\n",
      "weighted avg       0.84      0.68      0.73     20552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svms = []\n",
    "for i in range(splits):\n",
    "    svm = SVC(gamma='auto',class_weight='balanced')\n",
    "    svm.fit(X_train_splits[i],y_train_splits[i])\n",
    "    svms.append(svm)\n",
    "\n",
    "vc = VotingClassifier(svms)\n",
    "pred=vc.predict(X_test)\n",
    "print(\"Confusion matrix on test set:\\n\",sklearn.metrics.confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find C and gamma parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split 0\n",
      "Best combination of parameters found:  {'C': 10.0, 'gamma': 0.001}\n",
      "Recall 10-fold cross on train data = 0.502\n",
      "Number of supports:  3893 ( 0 of them have slacks)\n",
      "Prop. of supports:  0.8813674439664931\n",
      "\n",
      "Split 1\n",
      "Best combination of parameters found:  {'C': 100.0, 'gamma': 0.0001}\n",
      "Recall 10-fold cross on train data = 0.5279999999999999\n",
      "Number of supports:  3782 ( 0 of them have slacks)\n",
      "Prop. of supports:  0.856237265112067\n",
      "\n",
      "Split 2\n",
      "Best combination of parameters found:  {'C': 100.0, 'gamma': 0.0001}\n",
      "Recall 10-fold cross on train data = 0.5039999999999999\n",
      "Number of supports:  3783 ( 0 of them have slacks)\n",
      "Prop. of supports:  0.8564636631197645\n",
      "\n",
      "Split 3\n",
      "Best combination of parameters found:  {'C': 10.0, 'gamma': 0.001}\n",
      "Recall 10-fold cross on train data = 0.5640000000000001\n",
      "Number of supports:  3652 ( 0 of them have slacks)\n",
      "Prop. of supports:  0.8268055241113879\n",
      "\n",
      "Split 4\n",
      "Best combination of parameters found:  {'C': 10.0, 'gamma': 0.001}\n",
      "Recall 10-fold cross on train data = 0.5\n",
      "Number of supports:  3719 ( 0 of them have slacks)\n",
      "Prop. of supports:  0.8419741906271225\n",
      "\n",
      "Confusion matrix on test set:\n",
      " [[ 1232  1095]\n",
      " [ 5601 12624]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.53      0.27      2327\n",
      "           1       0.92      0.69      0.79     18225\n",
      "\n",
      "   micro avg       0.67      0.67      0.67     20552\n",
      "   macro avg       0.55      0.61      0.53     20552\n",
      "weighted avg       0.84      0.67      0.73     20552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Cs = np.logspace(-2, 2, num=5, base=10.0)\n",
    "gammas = [0.000001,0.00001, 0.0001,0.001,0.01,0.1,1,10]\n",
    "param_grid = {'C': Cs, 'gamma': gammas}\n",
    "\n",
    "svms = []\n",
    "for i in range(splits):\n",
    "    print(\"\\nSplit\",i)\n",
    "    \n",
    "    scorer = make_scorer(f1_score,pos_label=0)\n",
    "    grid_search = GridSearchCV(SVC(class_weight='balanced'), param_grid, cv=10, scoring=scorer, iid=True)\n",
    "    grid_search.fit(X_train_splits[i],y_train_splits[i])\n",
    "\n",
    "    parval=grid_search.best_params_\n",
    "    print(\"Best combination of parameters found: \",parval)\n",
    "    \n",
    "    scorer = make_scorer(recall_score,pos_label=0)\n",
    "    cvacc = cross_val_score(SVC(C=parval['C'],gamma=parval['gamma'],class_weight='balanced'), X=X_train_splits[i],  y=y_train_splits[i], cv=10, scoring=scorer)\n",
    "    print('Recall 10-fold cross on train data =', cvacc.mean())\n",
    "\n",
    "    svm = SVC(C=parval['C'],gamma=parval['gamma'],class_weight='balanced')\n",
    "    svm.fit(X_train_splits[i], y_train_splits[i])\n",
    "    svms.append(svm)\n",
    "    \n",
    "    print(\"Number of supports: \",np.sum(svm.n_support_), \"(\",np.sum(np.abs(svm.dual_coef_)==parval['C']) ,\"of them have slacks)\")\n",
    "    print(\"Prop. of supports: \",np.sum(svm.n_support_)/X_train_splits[i].shape[0])\n",
    "    \n",
    "vc = VotingClassifier(svms)\n",
    "pred=vc.predict(X_test)\n",
    "print(\"\\nConfusion matrix on test set:\\n\",sklearn.metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best values for recall of class 0 were obtained with linear kernel, polynomial kernel of degree 2 and RBF kernel. Now, we will look at the proportion of supports for each one of the experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear kernel average prop. of supports: 0.8595879556259906\n",
      "Polynomial kernel average prop. of supports: 0.8628028073352955\n",
      "RBF kernel average prop. of supports: 0.8525696173873669\n"
     ]
    }
   ],
   "source": [
    "linear_kernel_supports = (0.8988000905592031 + 0.8784242698664252 + 0.87706588182024 + 0.8467285487887707 + 0.7969209870953136) / 5\n",
    "print(\"Linear kernel average prop. of supports:\", linear_kernel_supports)\n",
    "poly_kernel_d_2_supports = (0.9533620104143083 + 0.8503509169119312 + 0.8523884989812089 + 0.8215983699343445 + 0.8363142404346842) / 5\n",
    "print(\"Polynomial kernel average prop. of supports:\", poly_kernel_d_2_supports)\n",
    "rbf_kernel_supports = (0.8813674439664931 + 0.856237265112067 + 0.8564636631197645 + 0.8268055241113879 + 0.8419741906271225) / 5\n",
    "print(\"RBF kernel average prop. of supports:\", rbf_kernel_supports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that RBF kernel generalises better. Now, we will repeat the process done with the RBF kernel, using the best values found for each SVM. This time we begin the process with the whole training dataset, and we will use the test dataset that came out from the preprocessing step for validation purposes, which was unused until this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 {0: 500, 1: 3916}\n",
      "Remaining {0: 7255, 1: 56834}\n",
      "Split 1 {0: 500, 1: 3916}\n",
      "Remaining {0: 6755, 1: 52918}\n",
      "Split 2 {0: 500, 1: 3916}\n",
      "Remaining {0: 6255, 1: 49002}\n",
      "Split 3 {0: 500, 1: 3916}\n",
      "Remaining {0: 5755, 1: 45086}\n",
      "Split 4 {0: 500, 1: 3916}\n",
      "Remaining {0: 5255, 1: 41170}\n",
      "\n",
      "Number of supports SVM0:  3648 ( 0 of them have slacks)\n",
      "Prop. of supports SVM0:  0.8260869565217391\n",
      "\n",
      "Number of supports SVM1:  3866 ( 0 of them have slacks)\n",
      "Prop. of supports SVM1:  0.8754528985507246\n",
      "\n",
      "Number of supports SVM2:  3796 ( 0 of them have slacks)\n",
      "Prop. of supports SVM2:  0.8596014492753623\n",
      "\n",
      "Number of supports SVM3:  3725 ( 0 of them have slacks)\n",
      "Prop. of supports SVM3:  0.8435235507246377\n",
      "\n",
      "Number of supports SVM4:  3737 ( 0 of them have slacks)\n",
      "Prop. of supports SVM4:  0.8462409420289855\n",
      "Confusion matrix on test set:\n",
      " [[ 1788  1621]\n",
      " [ 8039 17912]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.52      0.27      3409\n",
      "           1       0.92      0.69      0.79     25951\n",
      "\n",
      "   micro avg       0.67      0.67      0.67     29360\n",
      "   macro avg       0.55      0.61      0.53     29360\n",
      "weighted avg       0.83      0.67      0.73     29360\n",
      "\n",
      "Average prop. of supports: 0.8501811594202898\n"
     ]
    }
   ],
   "source": [
    "# Load train dataset\n",
    "df = pd.read_csv(\"Train.csv\")\n",
    "y_train=df['readmitted'].values\n",
    "X_train=df.values[:,0:71].astype('float32')\n",
    "# Load test dataset\n",
    "df2 = pd.read_csv(\"Test.csv\")\n",
    "y_test=df2['readmitted'].values\n",
    "X_test=df2.values[:,0:71].astype('float32')\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1)).fit(X_train)\n",
    "# Apply the normalization trained in training data in both training and test sets\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "ratio = counts[1]/counts[0]\n",
    "num_samples_0 = 500;\n",
    "num_samples_1 = int(num_samples_0*ratio)\n",
    "\n",
    "# Perform 5 undersamples of the train dataset\n",
    "rus = RandomUnderSampler(sampling_strategy={0:num_samples_0,1:num_samples_1},random_state=42)\n",
    "\n",
    "splits = 5\n",
    "X_train_splits = []\n",
    "y_train_splits = []\n",
    "\n",
    "for i in range(splits):\n",
    "    X_train_r, y_train_r = rus.fit_resample(X_train, y_train)\n",
    "    X_train_splits.append(X_train_r)\n",
    "    y_train_splits.append(y_train_r)\n",
    "    indices = rus.sample_indices_\n",
    "    X_train = np.delete(X_train,indices,0)\n",
    "    y_train = np.delete(y_train,indices,0)\n",
    "    unique, counts = np.unique(y_train_r, return_counts=True)\n",
    "    print(\"Split\",i,dict(zip(unique, counts)))\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    print(\"Remaining\",dict(zip(unique, counts)))\n",
    "\n",
    "# Train 5 SVMs with the best parameters found\n",
    "svms = []\n",
    "\n",
    "svm0 = SVC(C=10.0,gamma=0.001,class_weight='balanced')\n",
    "svm0.fit(X_train_splits[0],y_train_splits[0])\n",
    "print(\"\\nNumber of supports SVM0: \",np.sum(svm0.n_support_), \"(\",np.sum(np.abs(svm0.dual_coef_)==10.0) ,\"of them have slacks)\")\n",
    "svm0_prop_supp = np.sum(svm0.n_support_)/X_train_splits[0].shape[0]\n",
    "print(\"Prop. of supports SVM0: \",svm0_prop_supp)\n",
    "svms.append(svm0)\n",
    "\n",
    "svm1 = SVC(C=100.0,gamma=0.0001,class_weight='balanced')\n",
    "svm1.fit(X_train_splits[1],y_train_splits[1])\n",
    "print(\"\\nNumber of supports SVM1: \",np.sum(svm1.n_support_), \"(\",np.sum(np.abs(svm1.dual_coef_)==100.0) ,\"of them have slacks)\")\n",
    "svm1_prop_supp = np.sum(svm1.n_support_)/X_train_splits[1].shape[0]\n",
    "print(\"Prop. of supports SVM1: \",svm1_prop_supp)\n",
    "svms.append(svm1)\n",
    "\n",
    "svm2 = SVC(C=100.0,gamma=0.0001,class_weight='balanced')\n",
    "svm2.fit(X_train_splits[2],y_train_splits[2])\n",
    "print(\"\\nNumber of supports SVM2: \",np.sum(svm2.n_support_), \"(\",np.sum(np.abs(svm2.dual_coef_)==100.0) ,\"of them have slacks)\")\n",
    "svm2_prop_supp = np.sum(svm2.n_support_)/X_train_splits[2].shape[0]\n",
    "print(\"Prop. of supports SVM2: \",svm2_prop_supp)\n",
    "svms.append(svm2)\n",
    "\n",
    "svm3 = SVC(C=10.0,gamma=0.001,class_weight='balanced')\n",
    "svm3.fit(X_train_splits[3],y_train_splits[3])\n",
    "print(\"\\nNumber of supports SVM3: \",np.sum(svm3.n_support_), \"(\",np.sum(np.abs(svm3.dual_coef_)==10.0) ,\"of them have slacks)\")\n",
    "svm3_prop_supp = np.sum(svm3.n_support_)/X_train_splits[3].shape[0]\n",
    "print(\"Prop. of supports SVM3: \",svm3_prop_supp)\n",
    "svms.append(svm3)\n",
    "\n",
    "svm4 = SVC(C=10.0,gamma=0.001,class_weight='balanced')\n",
    "svm4.fit(X_train_splits[4],y_train_splits[4])\n",
    "print(\"\\nNumber of supports SVM4: \",np.sum(svm4.n_support_), \"(\",np.sum(np.abs(svm4.dual_coef_)==10.0) ,\"of them have slacks)\")\n",
    "svm4_prop_supp = np.sum(svm4.n_support_)/X_train_splits[4].shape[0]\n",
    "print(\"Prop. of supports SVM4: \",svm4_prop_supp)\n",
    "svms.append(svm4)\n",
    "\n",
    "vc = VotingClassifier(svms)\n",
    "pred=vc.predict(X_test)\n",
    "print(\"Confusion matrix on test set:\\n\",sklearn.metrics.confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Average prop. of supports:\", (svm0_prop_supp + svm1_prop_supp + svm2_prop_supp + svm3_prop_supp + svm4_prop_supp) / 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
